---
title: 'Forecasting internal migration with a state space model'
author: Trond

output:
  bookdown::pdf_book:
    toc: FALSE
   
bibliography: freq_bib.bib
---




```{r settings, include=FALSE, cache=FALSE}

## general settings
library(cbsodataR)
library(data.table)
library(ggplot2)
library(dlm)
library(xts)
library(tseries)
library(gridExtra)
library(rmarkdown)
library(knitr)
library(parallel)
library(forecast)
library(kableExtra)
library(xlsx)
library(wesanderson)

## set global rchunk options
opts_chunk$set(fig.path='../figs/freq--',
               cache.path='../cache/freq-',
#               fig.show='hold',
               results='hide',
               message=FALSE,
               warning=FALSE,
               echo=FALSE,
               par=TRUE )

## set English date settings
Sys.setlocale("LC_TIME", "C")

options(scipen = 999) # disable scientific notation

## a couple of links
## http://lenkiefer.com/2018/06/10/kalman-filter-for-a-dynamic-linear-model-in-r/
## https://otexts.com/fpp2/
## https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/
## https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python
## https://kevinkotze.github.io/ts-4-tut/
## http://gradientdescending.com/state-space-models-for-time-series-analysis-and-the-dlm-package/
## https://nwfsc-timeseries.github.io/atsa-labs/sec-dlm-forecasting-with-a-univariate-dlm.html
## https://towardsdatascience.com/state-space-model-and-kalman-filter-for-time-series-prediction-basic-structural-dynamic-linear-2421d7b49fa6
## http://people.duke.edu/~rnau/411arim3.htm
```

```{r read-data, cache = T}

## read data on house prices and costs
cbs_dt <- data.table(cbs_get_toc(Language="nl"))
## cbs_dt[grepl('verhuisde', Title, ignore.case = T) & grepl('maand', Title, ignore.case = T), .(Identifier, Title, ShortTitle, Modified)]

## cbs_dt[grepl('werklo', Title, ignore.case = T), .(Identifier, Title, ShortTitle, Modified)]

## wn_dt <- data.table(cbs_get_data('83906NED'))

## unemp_dt <- data.table(cbs_get_data('80479ned',
##                        Geslacht='T001038'
##                        ))

## cbs_get_meta('80479ned')
#cbs_get_meta('80509ned')
#cbs_get_meta('71450ned')

## stud_dt <- data.table(cbs_get_data('71450ned',
##                                    RegioS="NL01  ", ## country-level
##                                    Geslacht="T001038", ## gender-total
##                                    Leeftijd="10000")
##                       )

##cbs_get_meta('37943ned')
##load('../data/cbs_mig_data.RData')
bev_dt_hf <- data.table(cbs_get_data('37943ned'))
#cbs_get_meta('81734NED')
bev_dt_hist <- data.table(cbs_get_data('37556'))
##bev_dt_meta <- cbs_get_meta('84547NED')
mig_dt_hf2 <- data.table(cbs_get_data('84547NED',
                                      RegioS="NL01  ", ## country-level
                                      Geslacht="T001038", ## gender-total
                                      Leeftijd31December="10000") ## age-total
                         )
bev_dt_hf2 <- data.table(cbs_get_data('83474NED'))
```


```{r data-wrangling}

## subset and clean up
freq_dt <- bev_dt_hf[!grepl('2018', Perioden),
                     .(Perioden,
                     (as.numeric(TussenGemeentenVerhuisdePersonen_9) +
                      as.numeric(BinnenGemeentenVerhuisdePersonen_10)),
                     as.numeric(BevolkingAanEindVanGekozenPeriode_8),
                     year = substr(as.character(Perioden), 1, 4)
                     )
                     ]

setnames(freq_dt, c('month', 'moves', 'pop_ep', 'year'))

freq_dt2 <- merge(mig_dt_hf2[grepl('2018', Perioden) | grepl('2019', Perioden),
                     .(Perioden,
                     (as.numeric(BinnenGemeentenVerhuisdePersonen_1) +
                      as.numeric(GevestigdInDeGemeente_2)),
                     year = substr(as.character(Perioden), 1, 4))
                     ],
                 bev_dt_hf2[grepl('2018', Perioden) | grepl('2019', Perioden),
                           .(Perioden, BevolkingAanHetEindVanDePeriode_8)
                           ],
                 by = 'Perioden')
             
setnames(freq_dt2, c('month', 'moves', 'year', 'pop_ep'))

freq_dt <- rbind(freq_dt, freq_dt2)

## calculate frequency
freq_dt[, freq := 1000*moves/pop_ep]

## transform to long
freq_dt <- melt(
    freq_dt[grepl('MM', as.character(month)), -'year'],
    id.vars = 'month')[,
                       value := as.numeric(value)
                       ]

setkey(freq_dt, variable, month)

## create time series object: limit to December 2019
y_m <-  ts(freq_dt[variable == 'freq' & as.numeric(substr(month, 1, 4)) <= 2019,
                   value],
            start = c(1995, 1),
            frequency = 12         
           )

## limit variable to December 2018
## y_m <- window(y_m, end = 2018 + 11/12)

## u_m <- ts(unemp_dt[grepl('MM', Perioden) & Leeftijd == 53715, NietSeizoengecorrigeerd_7],
##             start = c(2003, 1),
##             frequency = 12         
##             )

## w_m <- ts(wn_dt[grepl('MM', Perioden), PrijsindexBestaandeKoopwoningen_1],
##             start = c(1995, 1),
##             frequency = 12         
##             )

## ggAcf(data.table(window(y_m, start = 2003), u_m)) + theme_bw()

## ggAcf(data.table(y_m, w_m)) + theme_bw()

## dcast(unemp_dt[grepl('MM', Perioden),.(Perioden, Leeftijd, NietSeizoengecorrigeerd_7)],
##       'Perioden ~ Leeftijd', value.var = 'NietSeizoengecorrigeerd_7')

## library(vars)

## VARselect(data.frame(y_m, w_m), lag.max = 10, type = 'const')
## var7 <- VAR(data.frame(y_m, w_m), p=7, type = 'const')
## var8 <- VAR(data.frame(y_m, w_m), p=8, type = 'const')
## serial.test(var8, lags.pt=10, type="PT.asymptotic")

## forecast(var7)

## ggplot(unemp_dt[grepl('MM', Perioden)],
##        aes(Perioden, NietSeizoengecorrigeerd_7, group = Leeftijd)) +
##     geom_line() +
##     facet_wrap(~Leeftijd) +
##     theme_bw()

## wn_dt[,.(Perioden, PrijsindexBestaandeKoopwoningen_1)]

## ggAcf(data.frame(y_m, w_m)) + theme_bw()

## ggAcf(merge(freq_dt[variable == 'freq'][,-'variable'],
##       unemp_dt[,.(Perioden, NietSeizoengecorrigeerd_7)],
##       by.x = 'month',
##       by.y = 'Perioden')) +
##     theme_bw()

## ggAcf(dcast(smoothed_dt[Gemeente == 'Amsterdam', .(Maand, Onderwerp, Trend)],
##             'Maand ~ Onderwerp')[,-'Maand']
##       ) +
##     theme_bw() +
##     ggtitle('Cross correlaties Amsterdam')

## write to csv file
## if (!file.exists('../data/mobility_frequencies.xlsx')) {
##     write.xlsx(freq_dt[variable == 'freq',
##                        .(time = time(y_m), frequency = value)],
##                '../data/mobility_frequencies.xlsx',
##                row.names = F
##                )
## }
          
```    

```{r dlm-functions}
source('../src/freq_dlm_fn_monthly.R')
```

```{r estimate-models-mle, cache = T}

## initial level of Kalman filter parameters
level0 <- log(y_m)[1]
slope0 <- mean(diff(log(y_m)))

## initiate lists for results
fit <- list()
freq_fit <- list()
freq_filt <- list()
freq_smooth <- list()

mods <- c('local_level', 'local_trend', 'lt_smooth',
          'lt_fourier', 'lt_arma_fourier', 'lt_arma',
          'lt_arma2', 'lt_arma3', 'lt_wn_reg')

## priors of parameters
init_arma2 <- c(-18, -9, -12, -6, -0.5, -0.4)
init_arma3 <- c(-18, -9, -12, -6, -0.5, -0.3, 0.1, 0.1)
init_reg_wn <- c(-18, -9, -12, rep(0, 2))

for (mod in mods[c(7:8)]) {
    message(paste0('Processing ', mod))
    if (mod == 'local_level' | mod == 'lt_smooth') {
        fit[[mod]] <- dlmMLE(log(y_m), par=rep(-1, 4), type = mod, build=freq_mod)
    }
    if (mod == 'local_trend' | mod == 'lt_fourier') {
        fit[[mod]] <- dlmMLE(log(y_m), par=rep(-4, 4), type = mod, build=freq_mod)
    }
    if (mod == 'lt_arma_fourier') {
        fit[[mod]] <- dlmMLE(log(y_m),
                             par=c(-0.1, -1, 0.1, 0.1, -4, rep(0.4, 2)),
                             type = mod,
                             build=freq_mod)
    }
    if (mod == 'lt_arma') {
        fit[[mod]] <- dlmMLE(log(y_m),
                             par=c(-0.1, -1, 0.1, 0.1, -4, rep(0.4, 2)),
                             type = mod, build=freq_mod)        
    }
    if (mod == 'lt_arma2') {
        fit[[mod]] <- dlmMLE(log(y_m),
                             par= init_arma2,
                             type = mod,
                             init_level = level0, init_slope = slope0,
                             build=freq_mod, hessian = T,
                             lower = c(-Inf, rep(-Inf,7))
                             )
    }
    if (mod == 'lt_arma3') {
        fit[[mod]] <- dlmMLE(log(y_m),
                             par=init_arma3,
                             type = mod,
                             init_level = level0, init_slope = slope0,
                             build=freq_mod,                             
                             hessian = T
                             )
    }
    if (mod == 'lt_wn_reg') {
        fit[[mod]] <- dlmMLE(log(y_m),
                             par=init_reg_wn,
                             type = mod,
                             init_level = level0, init_slope = slope0,
                             build=freq_mod,                             
                             hessian = T
                             )
    }    
    if (fit[[mod]]$convergence != 0) {
        message(paste0(mod, ' did not converge'))
    }
    freq_fit[[mod]] <- freq_mod(fit[[mod]]$par, type = mod,
                                init_level = level0, init_slope = slope0
                                )
    freq_filt[[mod]] <- dlmFilter(log(y_m), freq_fit[[mod]])
    freq_smooth[[mod]] <- dlmSmooth(freq_filt[[mod]])
}

## expand.grid(1:3, c('A', 'B', 'C'), c('D', 'E', 'F'))

## dlmForecast(freq_filt[[1]], nAhead = 2)

## qqnorm(window(residuals(freq_filt[[mods[8]]])$res
##               / residuals(freq_filt[[mods[8]]])$sd,
##               start = 2000))
## qqline(window(residuals(freq_filt[[mods[8]]])$res
##               / residuals(freq_filt[[mods[8]]])$sd,
##               start = 2000))

## plot(apply(dropFirst(freq_filt[[mod]]$m)[, 1:2], 1, sum))

## checkresiduals(residuals(freq_filt[[mods[2]]])$res)

## 
## tsdiag(freq_filt[['lt_arma3']])

## test for null hypothesis of independence
## https://robjhyndman.com/hyndsight/ljung-box-test/
## Box.test(residuals(freq_filt[[mods[8]]])$res,
##          lag=min(24, length(y_m)/5),
##          type="Ljung-Box")

## check autocorrelation of the residuals
## plot_dt <- residuals(freq_filt[[mods[8]]])$res
## grid.arrange(ggAcf(plot_dt) + theme_bw(),
##              ggPacf(plot_dt) + theme_bw()
##              )

## plot filtered values
## plot_dt <- rbindlist(lapply(mods[7],
##                             function(x)
##                                 if (x == 'lt_arma_reg') {
##                                     data.table(t = as.numeric(time(y_m)[-c(1:12)]),
##                                                           mod = x,
##                                            f = as.numeric(exp(freq_filt[[x]]$f)),
##                                            d = as.numeric(y_m[-c(1:12)])
##                                            )
##                                 } else {
##                                     data.table(t = as.numeric(time(y_m)),
##                                                           mod = x,
##                                            f = as.numeric(exp(freq_filt[[x]]$f)),
##                                            d = as.numeric(y_m)
##                                            )
##                                 }
##                             )
##                      )[t > 1997,
##                        m := mape(f, d),
##                        by = mod
##                        ]


```

```{r rolling-forecast, cache = TRUE}

## settings
n_cores <- detectCores() - 1
n_periods <- 18
break_point <- which(round(time(y_m), 3) == 2017.167)

##begin_fore <- break_point-n_periods+4
begin_fore <- break_point - 12
end_fore <- break_point+12
n_fore <- length(begin_fore:end_fore)

## estimated models
model_names <- sort(c('ARIMA', 'DLM1', 'DLM2', 'ETS', 'Holt-Winters', 'Naïve'))

## rolling forecast of dlm models
#dlm0_fore_out <- roll_fore_fn(begin_fore, end_fore, n_periods, 'dlm0', n_cores)
dlm1_fore_out <- roll_fore_fn(begin_fore, end_fore, n_periods, 'DLM1', n_cores)
dlm2_fore_out <- roll_fore_fn(begin_fore, end_fore, n_periods, 'DLM2', n_cores)

## arima model
arima_fore_out <- roll_fore_fn(begin_fore, end_fore, n_periods, 'ARIMA', n_cores) 

# tbats model
## tbats_fore_out <- roll_fore_fn(begin_fore, end_fore, n_periods, 'tbats', n_cores)

## ets model
ets_fore_out <- roll_fore_fn(begin_fore, end_fore, n_periods, 'ETS', n_cores)

## seasonal naive
naive_fore_out <- roll_fore_fn(begin_fore, end_fore, n_periods, 'Naïve', n_cores)

## naive with drift
hw_fore_out <- roll_fore_fn(begin_fore, end_fore, n_periods, 'Holt-Winters', n_cores)

## gather results
rf_dt <- rbindlist(lapply(1:n_fore,
                          function(x) {
                              rbindlist(list(#fore_plot_dt(dlm0_fore_out[[x]][[2]], plot = 'dlm0'),
                                             fore_plot_dt(dlm1_fore_out[[x]][[2]], plot = 'DLM1'),
                                             fore_plot_dt(dlm2_fore_out[[x]][[2]], plot = 'DLM2'),
                                             fore_plot_dt(arima_fore_out[[x]], plot = 'ARIMA'),
                                             fore_plot_dt(naive_fore_out[[x]], plot = 'Naïve'),
                                             fore_plot_dt(hw_fore_out[[x]], plot = 'Holt-Winters'),
                                             fore_plot_dt(ets_fore_out[[x]], plot = 'ETS')
                                        ))
                          }
                          )
                   )[,
                     mod := factor(mod, levels = unique(mod))
                     ][,
                       t := round(as.numeric(t), 3)
                       ]

## add actual values
rf_dt <- merge(rf_dt,
               data.table(t=round(as.numeric(time(y_m)), 3), actual_value=y_m),
               by = 't', all.x = T)

## calculate forecast error
rf_fit <- rbindlist(lapply(model_names,
                           function(x) {
                               rbindlist(lapply(1:n_fore,
                                      function(y) {
                                          dt <- as.numeric(window(y_m,
                                                       start = min(time(dlm1_fore_out[[y]][[2]]$f)),
                                                       end = max(time(dlm1_fore_out[[y]][[2]]$f))
                                                       ))
                                          if (x == 'DLM0') {
                                              pred <- as.numeric(exp(dlm0_fore_out[[y]][[2]]$f))                                     
                                          } else if (x == 'DLM1') {
                                              pred <- as.numeric(exp(dlm1_fore_out[[y]][[2]]$f))
                                          } else if (x == 'DLM2') {
                                                pred <- as.numeric(exp(dlm2_fore_out[[y]][[2]]$f))
                                          }else if (x == 'ARIMA') {
                                              pred <- as.numeric(exp(arima_fore_out[[y]]$mean))
                                          } else if (x == 'Naïve') {
                                              pred <- as.numeric(exp(naive_fore_out[[y]]$mean))
                                          } else if (x == 'Holt-Winters') {
                                              pred <- as.numeric(exp(hw_fore_out[[y]]$mean))
                                          } else if (x == 'ETS') {
                                              pred <- as.numeric(exp(ets_fore_out[[y]]$mean))
                                          }
                                          return(data.table(mape_6 = mape(dt[1:6], pred[1:6]),
                                                            rmse_6 = rmse(dt[1:6],pred[1:6]),
                                                            mape_12 = mape(dt[1:12], pred[1:12]),
                                                            rmse_12 = rmse(dt[1:12],pred[1:12]),
                                                            mape_18 = mape(dt, pred),
                                                            rmse_18 = rmse(dt,pred),
                                                            mod = unique(rf_dt$mod)[y],
                                                            type = x
                                                            )
                                                 )
                                      }
                                      )
                                      )
                           }
                           )
                    )

## two-year ahead forecast to calculate yearly frequency
two_y_origin <- 2016 + 11/12
two_year_fore <- rbindlist(mclapply(c('DLM2', 'ETS'),
                                    function(x) {
                                        orig <- round(two_y_origin, 3)
                                        t_num <- round(as.numeric(time(y_m)), 3)
                                        out <- roll_fore_fn(begin_f = which(t_num == orig),
                                                            end_f = which(t_num == orig),
                                                            periods = 24, mod = x, cores = 1)
                                        if (grepl('DLM', x)) {
                                            return(fore_plot_dt(out[[1]][[2]], plot = x))
                                        } else {
                                            return(fore_plot_dt(out[[1]], plot = x))
                                        }
                                    },
                                    mc.cores = 2
                                    ))[,
                                       ':='(
                                           t = round(t, 3),
                                           year = floor(t)
                                       )][is.na(forecast),
                                          ':=' (
                                             forecast = as.numeric(window(y_m, end = two_y_origin)),
                                             forecast_pl = as.numeric(window(y_m, end = two_y_origin)),
                                             forecast_pu = as.numeric(window(y_m, end = two_y_origin))
                                          )]


## add observations
two_year_fore <- merge(two_year_fore,
                       data.table(t=round(as.numeric(time(y_m)), 3),
                                  actual_value=y_m
                                  ),
                       by = 't', all.x = T)

## information about the selected ets model
## ets_fore_info <- ets(window(log(y_m), end = 2016 + 11/12))
## summary(ets_fore_info)
## coefficients(ets_fore_info)[1:4]
```

#Introduction

Long-term projections of internal migration are key inputs to regional-level demographic population projections. The long-term projections rely on the identification of a long-term trend in data, thus filtering out short-run fluctuations due to the business cycle. However, such detrending analyses are complicated if there is no clear trend in the data, if business cycles themselves stretch into the medium- to long run or if the data point represent the top or the bottom of the cycle [@canova1998detrending; @hamilton2018]. Therefore it is informative to analyse the short- to medium term dynamics of demographic components, forecasting along the business cycle. This paper develops a state space model for short-to medium term univariate forecast of monthly frequency of mobility in the Netherlands.

As is the case in most developed countries, the main driver of regional-level population change in the Netherlands is migration (external and domestic). Due to the close relation between migration decisions and labour- and housing market conditions, the volume of internal migration often moves along with the business cycle. However, recent research argues that the relationship between economic drivers and migration has changed [@kaplan2017understanding]. And other drivers than macroeconomic factors may be just as important in explaining migration - for example, family considerations are important determinants of migration decisions on an individual level, meaning migration flows could also be affected by changes to family composition and aging [see e.g. @mulder2018putting]. As such, there are many possible candidate determinants of internal migration and their influence on migration flows may change over time. 

Even if one succeeds in finding these determinants, their practical use in regional population projections may be somewhat limited. Many official population projections are carried out with cohort-component models, where the growth paths of the components are given as inputs to the model. A forecast of migration using explanatory variables requires knowledge of both the future trajectory of the explanatory variables and the future development of their relationship with migration. Consequently, in practice the growth path used as inputs in cohort-component models is most commonly based on extrapolation of long-term historic trends.

When working with the latest Dutch Regional Population Projections, published in September 2019 [@teriele2019], we were questioning whether the last data point in the time series of yearly frequency of mobility - registered at the end of 2016 - represented the top of a cycle. We knew, for example, that a recent change to the Dutch student finance system seems to have caused a decrease in mobility for an otherwise very mobile group.\footnote{https://www.cbs.nl/nl-nl/nieuws/2018/04/studenten-gaan-minder-op-kamers} 
As is common in the literature, key inputs for the cohort-component model used for the projections are extrapolations of the long-term trend of the components. Obviously, if the last data point used to determine the long-term trend represents the top of a cycle, our extrapolation could overestimate the true trend. In order to determine whether the last point of our yearly series of the migration frequency represented the top of a cycle, it was informative to carry out a medium-term forecast based on data which was both more recent and of higher frequency than the yearly series.

Forecasting of time series often involves decomposing the series into the more basic elements such as trend, cycle and seasonality. Univariate forecasting models make use of basic time series patterns to form a forecast [@zietz2014us]. One popular type of model used for this purpose are Dynamic Linear Models (DLM), which is a type of State Space models [@petris2009dynamic; @durbin2012time]. As the name suggests, DLMs are linear models with a Gaussian error structure, where the relevant inferences are carried out using the Kalman filter. Unknown parameters of the model are be estimated by either maximum likelihood or Bayesian techniques. One advantage of this model type is its modularity; time series components can be added to or removed from the model whenever deemed necessary. Another advantage is that state space models are stochastic, meaning one can derive forecast distributions, either analytically or by simulation. 

The time-series in this paper is the national-level frequency of mobility, defined as the sum of intra- and inter-municipal moves per 1000 inhabitants. This represents a measure of the intensity of internal migration in the Netherlands, and gives us a rough picture of migration also on a regional level. We compare the forecasting performance on our model with that of four other popular models for univariate time-series forecasting. These models are all fit using automatic routines available in *R*. As such, as a byproduct of the evaluation exercise, we evaluate the merits of our manual model selection compared to the automatic model selection in easy-to-use software packages.

In the paper I firstly show the time series data and discuss its patterns. Next I present the model, discuss its elements and the estimated parameters. Thereafter I evaluate the out-of-sample forecasting performance of the DLM, comparing it to that of four other models. Finally I discuss what the models tell us about the development of the frequency of mobility from 2017 onwards.

# Data

The frequency of mobility $y_{t}$ in month $t$ is in this paper defined as the sum of inter- en intramunicipality moves between the first and the last day of month $t$, divided by the population on the last day of $t$. Open data available from the Statistics Netherlands allow us to calculate monthly time series of the from January 1995 to December 2018, meaning we have in total `r length(y_m)` observations at our disposal. Figure \@ref(fig:freq-plot) shows the time series from January 1995 to December 2018 as well as the seasonal differences $y^{'}_{t} = y_{t} - y_{t-12}$.

```{r freq-plot, fig.cap = 'Frequency of mobility (upper panel) and seasonal differences'}

ggplot(melt(data.table(month = time(y_m),
                  `frequency of mobility` = y_m,
                  `seasonal difference` = c(rep(NA, 12), diff(y_m, 12))
                  ), id.vars = 'month'),
       aes(month, value)) +
    geom_line() +
    theme_bw() +
    ylab('') +
    xlab('') +
    facet_wrap(~variable, scales = 'free', ncol = 1) +
    #theme(axis.text.x = element_text(angle = 75, hjust = 1)) +
    scale_x_continuous(breaks = seq(1995, 2020, by = 5),
                       labels = paste('Jan', seq(1995, 2020, by = 5), sep = '\n')
                       )


```

The upper panel shows that there are between 11 and 7 moves per 1000 inhabitants per month. The plot suggests that the frequency of mobility is cyclical, without any clear trend. The time series is clearly non-stationary. The financial crisis, which hit the Dutch housing market hard, can be seen in the dip from 2008 onwards. We also see that the recovery from the crisis set in during the year of 2014. The lower panel, showing the seasonal differences, shows that the year on year changes are negative from January 2009 with a careful recovery in 2012 and a new dip in 2013 before a real recovery from 2014. 

Interestingly the lower-panel figure reveals that the year on year changes are negative from medio 2017. This is also reported by Statistics Netherlands:\footnote{\url{https://www.cbs.nl/nl-nl/nieuws/2018/26/minder-mensen-verhuisd-in-eerste-kwartaal-2018}} in 2018 there were 5 \% fewer moves than in 2017, with the decline concentrated primarily among people younger than 50 years old. This reflects thus an end of an increasing trend from 2014 onwards. It is precisely this apparent trend-change that we use to motivate this study.

In addition to the cycle, there are strong seasonal patterns that seem to vary over time. The seasonal subseries plot in Figure \@ref(fig:freq-month-plot) shows the movement per year and average per month (lower panel), and the horizontal lines in the figure indicating the means for each month. This plot enables us to see the underlying seasonal pattern clearly, and it also shows the changes in seasonality over time. The figure shows that the highest frequency is, on average, in August and July while the lowest is in April. We also see there is quite some variation between the years: the differences between the months were more pronounced in the early part of the time series than in later years. The lower panel suggests a sinusoidal pattern with two peaks within one year - one peak in the summer and one at the end of the year. 

```{r freq-month-plot, fig.cap = 'Seasonal subseries plot of the frequency of mobility'}

dat <- data.frame(y = as.numeric(y_m), year = trunc(time(y_m)),
                   season = as.numeric(cycle(y_m)))
seasonwidth <- (max(dat$year) - min(dat$year)) * 1.05
dat$time <- dat$season + 0.025 + (dat$year - min(dat$year))/seasonwidth
avgLines <- stats::aggregate(dat$y, by = list(dat$season),
                             FUN = mean)
#names(avgLines)[1] <- 'season'
colnames(avgLines) <- c("season", "avg")
dat <- merge(dat, avgLines, by = "season")
p <- ggplot2::ggplot(ggplot2::aes_(x = ~time, y = ~y,
                                   group = ~season), data = dat, na.rm = TRUE)
p <- p + ggplot2::theme(panel.grid.major.x = ggplot2::element_blank())
p <- p + ggplot2::geom_line()
p <- p + ggplot2::geom_line(ggplot2::aes_(y = ~avg),
                            col = wes_palette("IsleofDogs1")[6])
xfreq <- frequency(y_m)
xbreaks <- month.abb
p <- p + ggplot2::scale_x_continuous(breaks = 0.5 + (1:xfreq), 
                                     labels = xbreaks) +
    ylab('frequency of mobility') +
    xlab('month') +
    theme_bw()
p

## ggsubseriesplot(y_m) +
##     ylab('frequency of mobility') +
##     xlab('month') + 
##     theme_bw() +
##     ggtitle('') +
##     scale_colour_manual(values = wes_palette("IsleofDogs1", 2))

## grid.arrange(
##     ggplot(data.table(`frequency of mobility` = as.numeric(y_m),
##                                month = month.abb[round(as.numeric((time(y_m)-floor(time(y_m)))*12 + 1))],
##                                year = as.numeric(trunc(time(y_m)))
##                                )[, month := factor(month, levels = unique(month))
##                                  ],
##                     aes(month, `frequency of mobility`)) +
##              geom_violin() +
##              geom_jitter(aes(col = year), height = 0, width = 0, alpha = 0.4) +
##              scale_colour_viridis(option = 1, name = '') +
##              ylab('frequency of mobility') +
##              theme_bw() +
##              theme(legend.position = c(0.12, 0.88), legend.text = element_text(angle = 45, hjust = 0.7, size = 8), legend.direction = 'horizontal', legend.background = element_rect(fill ='transparent'))             
##              )         

``` 

Finally, we check whether there is a linear relationship between lagged variables of the time series (autocorrelation). Figure \@ref(fig:autocorr-plot) reveals a large and positive autocorrelation for small lags, since observations nearby in time tend to be similar in size. We also see that strong autocorrelation in lags that are multiples of the seasonal frequency (12, 24, and 36), which is due to the seasonality discussed above. The slow decline is related to the trend-cycle while the 'scalloped' pattern is related to the seasonality. The significant spikes at the first and second lag in the partial autocorrelation plot in the lower figure suggests that we should include at least two autoregressive terms.

```{r autocorr-plot, fig.cap = 'Autocorrelation function of the frequency of mobility'}

# plot acf with ggplot
gg_acf_pacf <- function(func = 'acf', series, n_lags) {
  significance_level <- qnorm((1 + 0.95)/2)/sqrt(sum(!is.na(series)))  
  ##a<-acf(series, n_lags, leplot=F)
  if (func == 'acf') {
      a<-ggAcf(series, lag = n_lags, plot = F)
  } else {
      a<-ggPacf(series, lag = n_lags, plot = F)
  }
  a.2<-with(a, data.frame(lag, acf))
  g<- ggplot(a.2[-1,], aes(x=lag,y=acf)) + 
      geom_bar(stat = "identity", position = "identity", width = 0.3) +
      xlab('lag') +
      ylab(toupper(func)) +
      geom_hline(yintercept=c(significance_level,-significance_level), lty=3) +
      theme_bw()
  # fix scale for integer lags
  if (all(a.2$lag%%1 == 0)) {
      g <- g +
          scale_x_discrete(limits = seq(0, n_lags, by = 6))
  }
  return(g);
}

grid.arrange(gg_acf_pacf('acf', y_m, 48),
             gg_acf_pacf('pacf', y_m, 48)
             )
```

Summing up, the decomposition of the time series suggests that there is no linear trend, rather a cycle with peaks roughly every 10 years. There are furthermore strong seasonal effects with variations between the years, meaning the seasonality contains noise. Furthermore, the seasonality and the trend-cycle lead to autocorrelation. The next section shows how these elements are adressed in the state space model.

# Model description

As mentioned in the Introduction, state space models are modular in the sense that the components discussed in the previous section can be included as separate components in the model. The essence of a state space model is that an unobserved dynamic process $\theta_{t}$ governs the observed $y_{t}$ which is a noisy measurement of the process. Forecasting and inference is done with the recursive Kalman filter algorithm. Residuals are calculated by comparing the one-step ahead predictions from the Kalman filter with the observed values. There are many ways of describing state space models - the model is implemented in *R* using the *dlm* package, therefore the description in this section follows the notation used in @petris2009dynamic.

Let $y_{t}$ denote the logarithm of the frequency of mobility in month $t$, where $y_{t}$ are the observed values of underlying (unobserved) vector of states $\theta_{t}$. The DLM assumptions are that errors are Gaussian and independent. We can write this DLM as:

$$
\begin{aligned}
& y_{t} = F \theta_{t} + v_{t} & v_{t} \sim N(0, V_{t}) \\
& \theta_{t} = G \theta_{t-1} + w_{t} &w_{t} \sim N(0, W_{t}) \\
& \theta_{0} \sim N(m_{0}, C_{0})
\end{aligned}
$$

In this DLM the observed frequency of migration, $y_{t}$, are conditionally independent given the state $\theta_{t}$: $P(y_{t} \bar \theta_{t})$. The state thus represents a latent Markov process, meaning that the probability of moving to the next state depends only on the previous state: $P(\theta_{t} \bar \theta_{t-1})$. The first equation is called the *observation equation* and the second the *state equation*. $v_{t}$ and $w_{t}$ are uncorrelated Gaussian errors, where the observation variances are gathered in the $m \times m$ matrix $V_{t}$ and system variances in the $p \times p$ matrix $W_{t}$. $F$ and $G$ are known system matrices of dimension $p \times m$ and $p \times p$ respectively. The initial $\theta_{0}$ are normally distributed with means $m_{0}$ and variances $C_{0}$.

Since the frequency of mobility is clearly non-stationary, we use as a starting point the linear growth or local linear trend model, defined as follows:

$$
\begin{aligned}
& y_{t} = \mu_{t} + v_{t} & v_{t} \sim N(0, \sigma_{v}^{2}) \\
& \mu_{t} = \mu_{t-1} + \beta_{t-1} + w_{t,1} &w_{t,1} \sim N(0, \sigma_{\mu}^{2}) \\
& \beta_{t} = \beta_{t-1} + w_{t,2} &w_{t,2} \sim N(0, \sigma_{\beta}^{2}) \\
\end{aligned}
$$

The local linear trend model is a DLM with $\theta_{t} = \mu_{t}$, where $y_{t}$ is modelled as a noisy observation of the level $\mu_{t}$. $\mu_{t}$ has a time-varying slope, governed by $\beta_{t}$. As is seen in this equation, we assume that the frequency of mobility is a non-stationary process growing with $\beta_{t-1}$. In line with the notation in @petris2009dynamic, we can write this as:

$$
\theta_{t} = \begin{bmatrix} \mu_t \\ \beta_{t} \end{bmatrix} \quad 
G_{t} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \quad 
F_{t} = \begin{bmatrix} 1 & 0 \end{bmatrix} \quad
W = \begin{bmatrix} \sigma_{\mu}^{2} & 0 \\ 0 & \sigma_{\beta}^{2} \end{bmatrix} \quad
V = \begin{bmatrix} \sigma_{v}^{2} \end{bmatrix}
$$

Seasonality is dealt with by including a trigonometric seasonal components to the model with a fixed periodicity of 12 months. One big advantage with the trigonometric specification, relative to simpler seasonal dummies, is that we allow autocorrelation to last through more lags, resulting in a smoother seasonal pattern. This way we filter out some of the noise seen in the figures in the previous section. By including non-zero variances the seasonal pattern is allowed to change trough time. Seasonality is incorporated in the DLM model by extending the state equation with an underlying periodic process $g_{t}$:
 
$$ 
\begin{aligned}
&\mu_{t} = \mu_{t-1} + g_{t} + v_{t} & v_{t} \sim N(0, \sigma_{v}^{2})
\end{aligned}
$$ 
 
where $g_{t}$ is defined as the sum of $j$ harmonics. For the $j$th harmonic, we can write the evolution of the seasonal effects as:

$$
\begin{aligned}
&S_{j, t+1} = S_{j, t} \cos \omega_{j} + S_{j, t}^{*} \sin \omega_{j} \\ 
&S_{j, t+1}^{*} = - S_{j, t} \sin \omega_{j} + S_{j, t}^{*} \cos \omega_{j} \\
\end{aligned}
$$

with $\omega_{j} = \frac{2 \pi t j}{s}$. We include two harmonics, meaning that the trigonometric specification can be written as:

$$
\theta_{t} = \begin{bmatrix} S_{1, t} \\ S_{1, t}^{*} \\ S_{2, t} \\ S_{2, t}^{*} \end{bmatrix} \quad
G = \begin{bmatrix} \cos \omega_{1} & \sin \omega_{1} & 0 & 0 \\ -\sin \omega_{1} & \cos \omega_{1} & 0 & 0 \\ 0 & 0 & \cos \omega_{2} & \sin \omega_{2} \\ 0 & 0 & -\sin \omega_{2} & \cos \omega_{2} \\ \end{bmatrix} \quad
F = \begin{bmatrix} 1 & 0 & 1 & 0\end{bmatrix} \\ \quad
W = \begin{bmatrix} \sigma_{S_{1}}^{2} & 0 & 0 & 0 \\ 0 & \sigma_{S_{1}^{*}}^{2} & 0 & 0 \\ 0 & 0 & \sigma_{S_{2}}^{2} & 0  \\ 0 & 0 & 0 & \sigma_{S_{2}^{*}}^{2} \end{bmatrix}
$$

Finally, residual autocorrelation not accounted for by the trend and seasonal components is dealt with by the introduction of an autoregressive elements into the model. The PACF from Figure \@ref(fig:autocorr-plot) suggested that an AR(2) model, of the following form, might be sufficient: 

$$
Y_{t} = \phi_{1} Y_{t-1} + \phi_{2} Y_{t-2} + u_{t} \quad u \sim N(0, \sigma^{2}_{u})
$$

In order to make sure that observations $Y_{t}$ are independent of past values of itself, an AR(2) model is written as a DLM by splitting up the state equation into two:

$$
\theta_{t} = \begin{bmatrix} \mu_{1,t} \\ \mu_{2,t} \end{bmatrix} \quad
G = \begin{bmatrix} \phi_{1} & 1 \\ \phi_{2} & 0 \end{bmatrix} \quad
F = \begin{bmatrix} 1 & 0 \end{bmatrix} \quad
W = \begin{bmatrix} \sigma_{u}^{2} & 0 \\ 0 & 0 \end{bmatrix}
$$

The resulting composite model (trend, seasonal and autoregressive) has a state vector $\theta_t$ of length 8: $\theta_t = (\mu_t, \beta_t, S_{1, t}, S_{1, t}^{*}, S_{2, t}, S_{2,t}^{*}, \mu_{1,t}, \mu_{2,t})'$. The model contains a number of unknown parameters that need to be estimated. Besides the observation variance $\sigma_{v}^{2}$, the matrix $W$ contains 6 potentially non-zero variances. We let $\sigma_{\beta}^{2} = 0$, which implies setting a constant nominal speed in the dynamics of $\mu_{t}$. This means that $\beta_{t}$ not only includes local trend growth, but also captures the global trend over the entire times series. In addition, a residual analysis revealed that a model where only the variance of the first harmonic is zero performs best. Finally, the $G$ matrix contains two parameters for the AR(2) part that need to be estimated. These parameters are estimated subject to stationarity restrictions: 

$$
-1 < \phi_{2} <1, \phi_{1} + \phi_{2} < 1, \phi_{2} - \phi_{1} < 1 
$$

Consequently there are in total 6 unknown parameters in the model: $\sigma_{v}^{2}$, $\sigma_{\mu}^{2}$, $\sigma_{S_{2}}^{2}$, $\sigma^{2}_{u}$, $\phi_{1}$ and $\phi_{2}$. These parameters are estimated using maximum likelihood [see @petris2009dynamic, ch. 4].

A residual analysis of the model described so far revealed that there was still some autocorrelation not captured by the seasonal effects (see Figure \@ref(fig:residual-autocorr) in the Appendix); namely in lags 7 and 12. These effects can be accounted for by including additional autoregressive terms to the state equation. Although this extension improves the model - residuals need to be Gaussian white noise in dynamic linear models- it is not a priori not clear that the forecasting accuracy is also improved. Since the inclusion of more autoregressive terms leads to a substantial extension of the state vector, estimation of such a model is much more costly. It is therefore useful to investigate whether this further refinements brings sufficient increase in performance. The model described above, which is in effect an AR(2) model is hereafter referred to as *DLM1*. We also estimate an extended model (*DLM2*) with 4 autoregressive terms. This model has two additional parameters that need to be estimated: $\phi_{7}$ and $\phi_{12}$.

After determining the values of unknown parameters we use the Kalman filter to obtain the filtered distribution of $\theta_{t}$ conditional on the observed series up to *t*, $\theta_{t} | y_{1}, y_{2},...,y_{t}$. In addition we can estimate a smoothing distribution of the components, representing the past values of the states given all observed values $\theta_{t} | y_{1}, y_{2},...,y_{s}$ where $s \ge t$. Forecasts of the next observation $Y_{t+1}$ based on observations up to $t$, $y_{t}$m are produced by first computing the state vector $\theta_{t+1}$ and then predicting $Y_{t+1}$. Similarly, an n-step ahead forecast $Y_{t+n}$ is based on calculation of the n-step ahead state vector $\theta_{t+n}$. We can write the forecast function as $f_{t} = E(Y_{t+n} | y_{1}, y_{2}, ....y_{t})$.

# Estimated model
The estimated parameters are shown in Table \@ref(tab:estimated-parameters). We see that the observation variance $\sigma_{v}^{2}$ is very close to zero, indicating a high level of precision on the observation. Note however, that the variances refer to the log of the frequency of mobility. The small size of the observation variance relative to the system variance results in a large signal-to-noise ratio W/V, meaning again that relatively large weight is put on prediction in the previous step in the Kalman algorithm. We furthermore see that both of the AR parameters are negative, with the absolute value of the parameter for the first lag larger than that of the second lag. The inclusion of two higher-order autoregressive elements in the DLM2 model results in a dampening of the effect of the two first autoregressive elements. Otherwise the parameter estimates are very similar.

```{r estimated-parameters, results='show', fig.cap = 'Estimated parameters'}

#Construct 95% confidence interval for the estimated measurement variance
##seParms <- sqrt(diag(solve(fit[[mod]]$hessian)))
##exp(fit[[mod]]$par[c(1:4)] + qnorm(.05/2)*seParms[c(1:4)]%o%c(1,-1))
##fit[[mod]]$par[5:6] + qnorm(.05/2)*seParms[5:6]%o%c(1,-1)

est_tbl <- data.table(par = c("$\\sigma_{v}^{2}$", "$\\sigma_{\\mu}^{2}$",
                              "$\\sigma_{S_{2}}^{2}$", "$\\sigma^{2}_{u}$",
                              "$\\phi_{1}$", "$\\phi_{2}$",
                              "$\\phi_{7}$", "$\\phi_{12}$"),
                           dlm1 = as.character(c(round(c(exp(fit[['lt_arma2']]$par[1:4]),
                                fit[['lt_arma2']]$par[5:6]), 8), rep('', 2)
                                )),
                           dlm2 = round(c(exp(fit[['lt_arma3']]$par[1:4]),
                                fit[['lt_arma3']]$par[5:8]
                                ), 8)
                      )

knitr::kable(est_tbl,
             booktabs = T, escape = F, digits = c(0, 6, 6),
             caption = 'Estimated variances and AR parameters'
             )
#    kable_styling(latex_options = "hold_position", full_width = F)

```

The one-step ahead predictions from the Kalman filter, or filtered values, from both of the models are displayed in Figure \@ref(fig:dlm-filtered), together with Root Mean Square Error (RMSE) calculated on basis of the residuals (one-step-ahead forecast errors). The adaptive nature of the Kalman algorithm causes the filtered values in the first periods to veer quite far off the observed values (not shown in the figure). We therefore discard the three first years when calculating the RMSE. It is clear from the figure that DLM2, with the two higher-order autoregressive terms, fits the data better than the DLM1 model, though the difference in terms of RMSE is not dramatic. A residual check showed that the normality assumption of dynamic linear models is justified for these models (see Appendix for a residual plot of DLM2).

```{r dlm-filtered, fig.cap = 'One-step-ahead predictions (solid line), observed data (dotted line) and RMSE (calculated from January 1998)'}

## apply(dropFirst(freq_filt[[mod]]$m), 1, sum)

## freq_filt[[mod]]$f
##varcovFilteredState <- dlmSvd2var(freq_filt[[mod]]$U.C, freq_filt[[mod]]$D.C)
##lapply(varcovFilteredState, function(x) apply(x, 1:2, sum))


## fsd <- exp(sqrt(unlist(freq_fore$Q)))
##         pl <- fore + qnorm(0.05, sd = fsd)
##         pu <- fore + qnorm(0.95, sd = fsd)

ggplot(rbindlist(list(filt_plot_dt(freq_filt[[mods[7]]])[t >= 1998][,mod:='DLM1'],
                filt_plot_dt(freq_filt[[mods[8]]])[t >= 1998][,mod:='DLM2']
                )),
       aes(t, y)) +
    geom_line(col = 'grey60') + 
    geom_line(aes(y = f)) +
    geom_text(aes(label = paste0('RMSE ', round(m, 4))),
              #family = "Times New Roman",
              x = 2012, y = 10, size = 3.5, col = 'grey40') +
    facet_wrap(~mod, ncol = 1) + 
    ylab('frequency of mobility') +
    xlab('month') + 
    scale_x_continuous(breaks = seq(1998, 2018, by = 5),
                       labels = paste('Jan', seq(1998, 2018, by = 5), sep = '\n')
                       ) + 
    theme_bw()
       
```

Figure \@ref(fig:ci-comparison) shows the 50 percent prediction intervals of the filtered values of DLM1 and DLM2 for the months between January 2010 and December 2018. The interval is calculated using the standard deviation of the filtered values [@petris2009dynamic, ch. 3]. It is clear from the figure that the incorporation of the higher-order autoregressive elements in DLM2 reduces the smoothness of the filtered values, which partly explains why DLM2 has a better fit than DLM1. We furthermore see that the prediction interval for the one-step ahead predictions of DLM2 is slightly smaller than that of DLM1.

```{r ci-comparison, fig.cap = "Comparison of prediction intervals between DLM1 and DLM2"}

ggplot(rbindlist(list(filt_plot_dt(freq_filt[[mods[7]]])[, mod := 'DLM1'],
          filt_plot_dt(freq_filt[[mods[8]]])[, mod := 'DLM2']
          ))[t > 2010], aes(t, f)) +
    geom_ribbon(aes(ymin = pl, ymax = pu), fill = 'grey') +
    geom_line() +
    facet_wrap(~mod, ncol = 1) +
    ylab('frequency of mobility') +
    xlab('month') + 
    scale_x_continuous(breaks = seq(2010, 2020, by = 2),
                       labels = paste('Jan', seq(2010, 2020, by = 2), sep = '\n')
                       ) + 
    theme_bw()

```

The smoothing estimates for the trend-cycle component and for the seasonal effects are shown in Figure \@ref(fig:dlm-smoothed). From the upper panel we can pin-point the recent peak in the trend-cycle to March 2017. The lower panel shows how the seasonal effects vary over time; at the start of the time series the within-year cycle exhibites one pronounced maximum (July) and one minimum (March). However, from around the middle of the series, there is gradually another local maximum within each year (Janauary). This means that the seasonal patterns in the model captures well the development of the seasonal factors from the raw data.

```{r dlm-smoothed, fig.cap = 'Smoothing estimate of the trend-cycle and seasonal effects'}

grid.arrange(
    ggplot(data.table(y = exp(apply(dropFirst(freq_smooth[[1]]$s[,c(1,2)]), 1, sum)),
                      t = time(y_m)
                      ), aes(t, y)) +
    geom_line() +
    ylab('frequency of mobility') +
    xlab('month') + 
    scale_x_continuous(breaks = seq(1995, 2020, by = 5),
                       labels = paste('Jan', seq(1995, 2020, by = 5), sep = '\n')
                       ) + 
    theme_bw(),
    ggplot(data.table(y = exp(apply(dropFirst(freq_smooth[[1]]$s[,c(3:6)]), 1, sum)),
                       t = time(y_m)
                       ), aes(t, y)) +
    geom_line() +
    ylab('frequency of mobility') +
    xlab('month') + 
    scale_x_continuous(breaks = seq(1995, 2020, by = 5),
                       labels = paste('Jan', seq(1995, 2020, by = 5), sep = '\n')
                       ) + 
    theme_bw(),
    ggplot(data.table(y = exp(apply(dropFirst(freq_smooth[[2]]$s[,c(1,2)]), 1, sum)),
                      t = time(y_m)
                      ), aes(t, y)) +
    geom_line() +
    ylab('frequency of mobility') +
    xlab('month') + 
    scale_x_continuous(breaks = seq(1995, 2020, by = 5),
                       labels = paste('Jan', seq(1995, 2020, by = 5), sep = '\n')
                       ) + 
    theme_bw(),
    ggplot(data.table(y = exp(apply(dropFirst(freq_smooth[[2]]$s[,c(3:6)]), 1, sum)),
                       t = time(y_m)
                       ), aes(t, y)) +
    geom_line() +
    ylab('frequency of mobility') +
    xlab('month') + 
    scale_x_continuous(breaks = seq(1995, 2020, by = 5),
                       labels = paste('Jan', seq(1995, 2020, by = 5), sep = '\n')
                       ) + 
    theme_bw()    
)

```

# Cross validation: evaluation over a rolling forecasting origin

In this section we present the results of the evaluation of forecasting accuracy of the models described in the previous sections in comparison to four other popular time-series models. The evaluation method is similar to out-of-sample testing in cross-sectional data, where a model is estimated on a training data set and evaluated on an independent test data set. In the time-series setting is interesting to evaluate how performance varies with the length of the forecast horizon, whereby accuracy measures are calculated for several forecast lengths (in effect N-step forecast). In order to control for effects arising from the composition of the training data, we carry out the evaluation using a k-fold cross validation approach [@hastie2009]. Due to the serial autocorrelation and potential non-stationarity in the data, it is common to choose a test set that does not contain observations occuring prior to the observations in the training data [@bergmeir2012use]. Recent literature suggests that standard cross-validation can be applied to certain time series models [@bergmeir2018] - however, due the Markovian structure of the Kalman filtering algorithm, we suspect that the technique would not work for the state space models discussed in the previous section. Therefore we follow the conventional approach for evaluating time series accuracy; namely evaluation over a \textit{rolling forecasting origin} [@tashman2000out], where we succesively extend the training data and produce forecasts of the test data. The last point of the training data in each iteration is referred to as the \textit{origin} $T$, since the origin on which the forecast is based rolls forward in time. In this approach, the training data consists of observations between $t = 1, ..., T$ and forecasts are generated for time periods $T+1$, $T+2$, ..., $T+N$. This procedure is illustrated in Figure \@ref(fig:rolling-forecast-example). The movement along the x-axis shows how the origins (vertical dotted line) "roll" forward in time, where the model is estimated on the training data (black solid line). The model perfomance is then evaluated by averaging the prediction errors over different forecast horizon (dotted horizontal lines).

```{r rolling-forecast-example, fig.cap = "Illustration of rolling forecasting origin: training data (solid black line), origin (vertical dark grey line), forecast horizion (dotted horizontal lines) and test data (grey line)"}

plot_dt <- melt(rbindlist(lapply(seq(2016, 2016.75, by = 0.25),
                                 function(x) {
                                     origin_month  <- c('Jan', 'Apr',
                                                        'Jul', 'Oct'
                                                        )[which(seq(2016,
                                                                    2016.75,
                                                                    by = 0.25) == x)
                                                          ]
                                     data.table(origin = factor(paste0('origin ',
                                                                origin_month,
                                                                ' 2016')),
                                                dt = y_m,
                                                t = time(y_m),
                                                xint = x,
                                                train = c(window(y_m, end = x),
                                                          rep(NA, length(window(y_m, start = x + 1/12)))
                                                          ),
                                                `n=6` = c(rep(NA, length(window(y_m, end = x))),
                                                       rep(7, 6),
                                                       rep(NA, length(window(y_m, start = x + 7/12)))
                                                       ),
                                                `n=12` = c(rep(NA, length(window(y_m, end = x))),
                                                        rep(7.3, 12),
                                                        rep(NA, length(window(y_m, start = x + 13/12)))
                                                        ),
                                                `n=18` = c(rep(NA, length(window(y_m, end = x))),
                                                        rep(7.6, 18),
                                                        rep(NA, length(window(y_m, start = x + 19/12)))
                                                        )
                                                )
                                 })),
                id.vars = c('origin', 'dt', 't', 'train', 'xint'),
                variable.name = 'horizon'
                )

ggplot(plot_dt[t %between% c(2014, 2018)], aes(t, dt)) +
    geom_line(col = 'grey') +
    geom_line(aes(y = train), col = 'black') +
    geom_line(aes(y = value, lty = horizon)) +
    facet_wrap(~origin, ncol = 1) +
    geom_vline(aes(xintercept = xint), col = 'grey40') + 
    theme_bw() +
    scale_x_continuous(breaks = seq(2014, 2018),
                       labels = paste('Jan', seq(2014, 2018), sep = '\n')
                       ) + 
    ylab('frequency of mobility') +
    xlab('month')

## plot_dt <- rbindlist(lapply(2:6,
##                  function(x)
##                      data.table(mod = x,
##                                 origin = c(rep(NA, x-1), x, rep(NA, 10-x)),
##                                 train = c(seq(1, x), rep(NA, 10 - x)),
##                                 test = c(rep(NA, x), seq(x+1, 10)),
##                                 horizon = c(rep(NA, x), seq(x+1, x+2), rep(NA, 10 - x -2))
##                                 )
##                  )
##           )

## pal <- brewer.pal(3, name = 'Set1')

## ggplot(plot_dt, aes(mod, train)) +
##     coord_flip() + 
##     geom_point(col = pal[2], size = 4) +
##     geom_point(aes(y = test), colour = 'grey', size = 4) +
##     geom_point(aes(y = horizon), shape = 21, colour = 'black', fill = 'grey', size = 4, stroke = 2) +
##     geom_point(aes(y = origin), shape = 21, fill = pal[2], colour = pal[1], size = 4, stroke = 2) +
##     theme_bw() +
##     theme(
##         axis.text.x=element_blank(),
##         axis.ticks.x=element_blank(),
##         axis.title.y=element_blank(),
##         axis.text.y=element_blank(),
##         axis.ticks.y=element_blank()) +
##     ylab('Time') + 
##     scale_x_reverse()
    

```

We evaluate forecasting performance on three time horizons - $N=6$, $N=12$ and $N=18$ - meaning that, for each origin, we calculate three indicators of accuracy; one for a half year forecast, one for a year and one for one and a half year. We are interested in assessing the performance around the changepoint of the trend-cycle. From the previous section we identified this as March 2017 (see Figure \@ref(fig:dlm-smoothed)). In order to evaluate the performance of the models on both sides of the changepoint we include origins within 12 months before and after March 2017. Including the changepoint itself we have in total 25 forecasting origins: T = March 2016, February 2016, ..., June 2017.

In order to ensure independence between the test and training sample, we estimate the models on the relevant training data of each origin forecast. As such, it is possible that estimated parameter values differ between the different origins. The resulting mean and standard deviations, shown in Table \@ref(tab:par-comparison) in the Appendix, reveal that differences in training data has a very limited effect on the estimated parameters. This also gives us confidence that there is enough data for estimation even on the first origin.

Performance of the DLMs is assessed by comparing forecasting accuracy with that of three other popular models. The first of these is a naïve seasonal model (Naïve), where the forecast of a specific month is simply the value of the same month of the previous year. Despite its simplicity this model is useful with data with no clear trends. Since the model assumes no trend, we expect this model to perform particularly well around the changepoint. The next model is the Holt-Winters method with multiplicative seasonality [@holt2004forecasting; @winters1960forecasting]. This model is an extention of the simple exponential smoothing model, allowing for forecasts with a trend. Using the function ${\tt HoltWinters()}$ in base *R*, the smoothing parameters of the model are selected automatically, for each origin, with the default initial values. This model type performs particularly well on data with a clear trend, and we therefore expect it to forecast accurately before the changepoint and increasingly worse around and after it. Furthermore, exponential smoothing can also be formulated on state space form [@hyndman2018forecasting]. An advantage of casting exponential smoothing models on the stochastic state space form is the possibility of generating prediction intervals - either analytically or by simulation. This furthermore allows information criteria to be used for model selection, meaning that all combination of trend-, seasonal and error components can be explored [@hyndman2002state]. The automatic model selection is carried out using the ${\tt ets()}$ function with default values for all arguments, meaning that selection is based on the corrected Akaike's Information Criterion (AICc). Finally we include a seasonal ARIMA model into the comparison. Identification of the model is carried out using the ${\tt auto.arima()}$ function on the whole sample [@hyndman2019], and results in an ARIMA(2,1,5)(2,1,1). This is thus a model with double differencing, with yearly and monthly MA(2), and yearly AR(5) and monthly AR(2). For each origin we reestimate the model parameters on the respective training data. We also tried the automatic selection for each origin with only marginal differences in results.

Table \@ref(tab:error-comparison) shows the overall forecast accuracy across all origins for all models. We choose a similar approach as the k-fold cross-validation discussed in @hastie2009, investigating the distribution of accuracy accross "folds" (in this case forecasting origins). The table shows both RMSE and mean absolute percentage error (MAPE), averaged over origins, of each of forecasting horizon (6 months, 12 months, 18 months). In order to asses whether the accuracy varies between origins the table also shows the standard deviation of RMSE and MAPE. From the table we see that both RMSE and MAPE of the DLM1 and DLM2 are quite similar across all forecast horizons, however with a slight advantage of DLM2. Not suprisingly, accuracy deteriorates as N increases, although not dramatically so: in the case of DLM2, the mean RMSE for N=6 is 0.39, compared to 0.53 for N=18. The limited deterioration in forecast accuracy is not shared with the ARIMA and Holt-Winters models, which both see substantial increases in forecasting errors as the forecast horizon increases. The mean RMSEs of the Naïve model is remarkably stable across all forecast horizons, and it is, in general, the least accurate model. We see that the ETS model outperforms DLM2 for the shortest forecast horizon in terms of both RMSE and MAPE. For forecasts of 12 or 18 months the ETS model performs slightly better than DLM2 in terms of RMSE and slightly worse in terms of MAPE. Since squared errors gives a heavier penalty to large forecast errors than absolute errors, this means that DLM2 produces the smallest median forecast error and ETS the smallest mean forecast error.

```{r error-comparison, results = 'show'}

rmse_mean <- rbindlist(
    list(err_dt('rmse_6')[26:27],
         err_dt('rmse_12')[26:27],
         err_dt('rmse_18')[26:27]
         ))

mape_mean <- rbindlist(
    list(err_dt('mape_6')[26:27],
         err_dt('mape_12')[26:27],
         err_dt('mape_18')[26:27]
         ))

error_dt <- cbind(t(data.frame(t(c('RMSE', rep('', nrow(rmse_mean)))),
                               t(c('MAPE', rep('', nrow(rmse_mean)-1)))
                               )),
                  rbindlist(list(rmse_mean,
                                 data.table(t(data.frame(rep('', ncol(rmse_mean))))),
                                 mape_mean)
                            ))
      
setnames(error_dt, c('', 'Forecast horizon', model_names))

## rmse_mean <- lapply(c(1, 3, 5),
##                    function(x) {
##                        dt <- t(rmse_mean)[-1, c(x, x+1)]
##                        out <- data.frame(paste0(dt[,1], '(' ,dt[,2], ')'))
##                        rownames(out) <- rownames(dt)
##                        names(out) <- gsub('Mean ', 'N=', t(rmse_mean)[1, c(x, x+1)][1])
##                        return(out)
##                    })

##knitr::kable(cbind(rmse_mean[[1]], rmse_mean[[2]], rmse_mean[[3]]),
knitr::kable(error_dt,
             booktabs = T, escape = F, digits = 4,
             caption = 'Mean (and standard deviation) of RMSE and MAPE'
             )

```


Figure \@ref(fig:rmse-origins) delves further the variation in forecasting performance across origins, showing the MAPE per origin and model for the three forecast horizons. In all forecast horizions the two DLMs are the best performing models in the interval between December 2016 and December 2017, with the exception of origins right after March 2017 where the Naïve model perfoms best. We see that the ARIMA, Holt-Winters and the ETS models all perform better than the DLMs on the part of the time series where there is a clear trend in the training data that at least partially extends into the test data (until December 2016). As Table \@ref(tab:error-comparison) showed, the higher forecast accuracy of the ETS model relative to the DLMs occurs primarily for a short forecast horizon. The relatively high MAPE for the DLMs prior to December 2016. Finally, the difference in performance between the two DLMs is, in general, minimal. However, the 6 month forecasts from medio 2017 onwards differ somewhat between the models, with DLM2 performing better than the DLM1. 

```{r rmse-origins, fig.cap = "MAPE by origin and forecast horizon"}

out <- rbindlist(list(err_dt('mape_6')[-c(26:27)],
                 err_dt('mape_12')[-c(26:27)],
                 err_dt('mape_18')[-c(26:27)]
                 ))[,
                   N := rep(paste0('N=', c(6, 12, 18)), each = n_fore)
                   ][,
                     N := factor(N, levels = c(paste0('N=', c(6, 12, 18))))
                     ][,
                       c(model_names) := lapply(.SD, function(x) as.numeric(as.character(x))),
                       .SDcols = model_names
                       ]

##library(stringr)
##unlist(str_split(as.character(out$Origin), '[[:digit:]]{4} '))

orig_nms <- sapply(unique(out$origin)[seq(1, 25, by = 3)],
                   function(x) {
                       paste(substr(x, 6, 8),
                             substr(x, 1, 4),
                             sep = '\n')
                   })
           
setnames(out, c('origin', model_names, 'N'))

ggplot(melt(out,
            id.vars = c('origin', 'N'), variable.name = 'model'
            ),
       aes(origin, value, group = model)) +
    geom_line(aes(col = model)) +
    ##geom_col(aes(fill = model), position = 'dodge2') + 
    ##coord_flip() + 
    facet_grid(N~., scale = 'free') +
    theme_bw() +
    ylab('MAPE') +
    ##scale_colour_brewer(palette = 'Set2') + 
    scale_colour_manual(values = wes_palette("IsleofDogs1")) + 
    scale_x_discrete(breaks = unique(out$origin)[seq(1, 25, by = 3)], 
                     labels = orig_nms)
#    theme(legend.position = 'bottom')

```

Figure \@ref(fig:rmse-origins) showed how the forecasting performance varied between origins; it suggested that the DLMs performed poorly, relative to other models, where a trend carried over from the training data to the test data and good relative to other models otherwise. Of course our initial interest in this exercise was motivated by a potential trend change. Therefore we zoom in on the forecasted values of the frequency of mobility around the changepoint. Figure \@ref(fig:forecast-2017-2018) compares the forecasts of all models in the period January 2017 to December 2018. The grey dots represent forecast from different origins, the solid line represents the average value of the forecasts across all origins and the dotted line is the data. The figure shows clearly the divergence in forecasting performance between the DLMs and the ARIMA and Holt-Winters models. The Holt-Winters model systematically overpredicts the migration frequency already from June 2017, meaning that the forecasted frequency exceeds the data for all origins. For the ARIMA model this occurs from October 2017. In the case of the DLMs the last forecasted point with a value lower than that of the data occurs in May 2018.

```{r forecast-2017-2018, fig.cap = "Forecasted (light grey dots), forecast averaged over origins (black line) and observed (dark grey line) frequency of mobility"}


ggplot(rf_dt[t >= 2017 & t < 2019,
             .(forecast, actual_value, mean(forecast, na.rm = T)),
             by = c('t', 'type')],
       aes(t, forecast)) +
    geom_point(col = 'grey90') +
    ##geom_smooth(aes(y=value), method = 'loess') +
    geom_line(aes(y = V3)) +
    geom_line(aes(y=actual_value), col = 'grey50') +
    facet_wrap(~type) +
#    scale_colour_viridis(discrete = T) +
    theme_bw() +
    scale_x_continuous(breaks = seq(2017, 2018 + 11/12, by = 4/12),
                       labels = unique(unlist(lapply(c(2017, 2018),
                                              function(x) {
                                                  paste(rep(c('Jan', 'May', 'Sep'), 2), x, sep = '\n')
                                              })
                                       ))
                       ) +
    ylab('frequency of mobility') +
    xlab('month')


```


# Forecasting migration frequency from January 2017 onwards

As mentioned in the Introduction, the realisation that 2017 possibly represented a turning point in the cycle of the frequency of mobility triggered our initial interest in developing the state space model. Then it is relevant to ask what the second state space model tell us about the development of the frequency of migration from 2017 onwards. In our case we were particularly interested in the development of the yearly frequency the coming couple of years. Figure \@ref(fig:forecast-2017) shows a two-year forecast of both the monthly and yearly frequency of mobility with their 95 percent prediction intervals, where the last period of the training data is December 2016. For simplicity, the yearly frequency is calculated by aggregating the monthly frequencies within that year. As a comparison, we also show results the same forecast using the ETS model discussed in the previous section. The automatic selection procedure results in an ETS(M, Ad, A): a model with multiplicative errors, damped additive trend and additive seasonal effects.

```{r forecast-2017, fig.cap = "Two-year forecast (black line) of monthly (left) and yearly (right) frequency of mobility and test data (dark grey line) and 95 percent prediction interval"}

p1 <- ggplot(two_year_fore[t >= 2010, ], aes(t, actual_value, group = type)) +
    geom_ribbon(aes(ymin = forecast_pl, ymax = forecast_pu),
                fill = 'grey80', alpha = 0.4) +
        geom_line(col = 'grey50') +
    geom_line(aes(y = forecast)) +
    scale_colour_manual(values = ) + 
    facet_wrap(~type, ncol = 1) +
    theme_bw() +
    ylab('frequency of mobility (monthly)') +
    xlab('month') +
    scale_x_continuous(breaks = seq(2010,2019, by = 2),
                       labels = paste('Jan',
                                      seq(2010,2019, by = 2),
                                      sep = '\n'))

p2 <- ggplot(two_year_fore[,
              .(actual = sum(actual_value),
                forecast = sum(forecast),
                upper_bound = sum(forecast_pl),
                lower_bound =  sum(forecast_pu)
                ),
              by = c('year', 'type')
              ][year >= 2010], aes(year, actual)) +
    geom_ribbon(aes(ymin = lower_bound, ymax = upper_bound),
                fill = 'grey80', alpha = 0.4) + 
    geom_line(col = 'grey50') +
    geom_line(aes(y = forecast)) +
    facet_wrap(~type, ncol = 1) +
    theme_bw() +
    ylab('frequency of mobility (yearly)') +
    xlab('year')

grid.arrange(p1, p2, ncol = 2)

## ggplot(rbindlist(list(filt_plot_dt(freq_filt[[mods[7]]])[, mod := 'DLM1'],
##           filt_plot_dt(freq_filt[[mods[8]]])[, mod := 'DLM2']
##           ))[t > 2010], aes(t, f)) +
##     geom_ribbon(aes(ymin = pl, ymax = pu), fill = 'grey') +
##     geom_line() +
##     facet_wrap(~mod, ncol = 1) +
##     ylab('Log(Frequency of mobility)') +
##     theme_bw()

```

At first glance, the point forecast of the monthly frequency (left part of the figure) is quite similar across models, although the prediction interval of ETS is smaller. Both models forecast 2017 accurately, and they both systematically overestimate the test data in 2018. As the right left panels of Figure \@ref(fig:forecast-2017) shows, the yearly frequency of mobility actually declines between 2017 and 2018, and none of the models were able to capture this sudden decline. However, we see ETS is much more "positive" about (i.e., overestimates more substantially) the yearly frequency than DLM2. Figure \@ref(fig:rmse-origins) already hinted at the reason: we saw that the DLM performed worse than the other models, and especially for short forecast horizons on the first origins in 2016. Since the autoregressive terms included in the model are all lags within one year, long-term forecasts (above one year) are primarily driven by the linear growth component. As we recall from above, in the linear growth or local linear trend model, $\beta_t$ is the time-varying slope in the dynamics of the local level $\mu_{t}$. The recursive updating of this parameter implies the assymptotic dynamics of the slope parameter is given by its value in the previous step and the forecasting error weighted by the signal-to-noise ratio (or, more precisely, the Kalman gain) [@petris2009dynamic, eq. 3.11]. Figure \@ref(fig:trend-growth) in the Appendix shows how the estimated $\hat{\beta}_{t}$ develops between 2000 and end 2016. We see immediately that the parameter is negative in most of the interval in the figure, but that it becomes slightly positive towards the end of 2016. Since n-step-ahead forecasts using *dlm* are based on time-invariant values of all parameters, the $\hat{\beta}_{t}$ used for the forecast in Figure \@ref(fig:forecast-2017) is slightly larger than zero. 

# Conclusion

In this paper we have developed a state space model for short-to medium term univariate forecast of monthly frequency of mobility in the Netherlands. The frequency of mobility is defined as the number of internally migrated persons countrywide (within- and between-municipality) per month divided by the population. This is an important input into the cohort-component model used for our regional population projections; the need for this modelling exercise arose as we suspected that the end of our yearly series represented the top of a cycle.

The results presented in the previous section show that the state space model (DLM) quite accurately forecasts both monthly and yearly frequency of mobility until end 2018, based on data up to end 2016. In particular, the model would have given us a very accurate picture of the development of the frequency of mobility until end of 2017. More generally, the model performed comparably or even better than a number of popular univariate forecasting in a time interval between end of 2016 to end of 2017. One of the most remarkable feature about the forecasts of the DLM was the relatively limited deterioration in forecast accuracy as the forecast horizon increased. 

The most important reason was that the local linear growth component ensured a certain "conservatism" for forecast horizons above 12 months. The high signal-to-noise rate implied that the estimated local growth rate evolved slowly over time. The only other model mimicking this behaviour was the ETS model - itself also a state space model. An easy critique of this result is that a model that predicts that 'what goes up must eventually come down' is of limited value. Yet such criticism misses the point that the local linear growth component in effect incorporates not only local, but to some extent global, characteristics of the time series. As such, this model also answers the question "come down to what?".

Although the focus of this paper was univariate forecasts, one of the biggest advantages of the structural model developed in this paper, relative to automatically selected models such as ETS, is the easy with which covariates can be included. Future research should focus on investigating whether model performance can be improved by including covariates for which there are reliable short- to medium-term forecasts - for example unemployment levels. Splitting up the time series into several age groups (dynamic hierarchical models) will probably make such an exercise easier, and is probably a fruitful venue for future research. A more theory-driven model with covariates could also provide extra information about the merits of the simpler univariate approach presented.

# Appendix

```{r residual-autocorr, fig.cap = "Autocorrelation function of the standardised residuals"}
##checkresiduals(residuals(freq_filt[[mods[7]]])$res)
## grid.arrange(ggAcf(residuals(freq_filt[[mods[7]]])$res) + theme_bw() + ggtitle('DLM1'),
##              ggAcf(residuals(freq_filt[[mods[8]]])$res) + theme_bw() + ggtitle('DLM2')
##              )

grid.arrange(
    gg_acf_pacf(func='acf',
                series=residuals(freq_filt[['lt_arma2']])$res, n_lags = '24') +
    ggtitle('DLM1'),
    gg_acf_pacf(func='acf',
                series=residuals(freq_filt[['lt_arma3']])$res, n_lags = '24') +
    ggtitle('DLM1')
)
    

```

```{r qqnorm, fig.cap = "Normal probability plot of standardized one-step-ahead forecast errors of DLM2"}
## df <- data.frame(y = as.numeric(window(residuals(freq_filt[[mods[8]]])$res
##                                        / residuals(freq_filt[[mods[8]]])$sd,
##                                        start = 2000)
##                                 )
##                  )
df <- data.frame(y = residuals(freq_filt[[mods[8]]])$res)

ggplot(df, aes(sample = y)) +
    stat_qq() +
    stat_qq_line() +
    theme_bw()

## qqnorm(window(residuals(freq_filt[[mods[8]]])$res
##               / residuals(freq_filt[[mods[8]]])$sd,
##               start = 2000))
## qqline(window(residuals(freq_filt[[mods[8]]])$res
##               / residuals(freq_filt[[mods[8]]])$sd,
##               start = 2000))
```


```{r par-comparison, results = 'show'}
roll_pars <- rbindlist(lapply(1:n_fore,
                         function(x) {
                             data.table(origin = time(y_m)[begin_fore + x - 1],
                                        dlm1 = c(exp(dlm1_fore_out[[x]][[1]][c(1:4)]),
                                                 dlm1_fore_out[[x]][[1]][c(5:6)],
                                                 rep(NA, 2)),
                                        dlm2 = c(exp(dlm2_fore_out[[x]][[1]][c(1:4)]),
                                              dlm2_fore_out[[x]][[1]][c(5:8)]
                                          )
                                        )[,
                                          V3 := est_tbl$par
                                          ]
                         })
                  )[,
                    .(as.character(round(mean(dlm1, na.rm = T), 8)),
                      as.character(round(sd(dlm1, na.rm = T), 8)),
                      as.character(round(mean(dlm2), 8)),
                      as.character(round(sd(dlm2), 8))),
                    by = V3
                    ][7:8,
                      c('V1', 'V2') := ''
                      ]
setnames(roll_pars, c('Parameter', 'DLM1 Mean', 'DLM1 SD', 'DLM2 Mean', 'DLM2 SD'))
knitr::kable(roll_pars,
             booktabs = T, escape = F, digits = c(0, 4, 4, 4, 4),
             caption = 'Mean and standard deviation of the estimated parameters')
```

```{r trend-growth, fig.cap = "The local growth rate of DLM2 ($\\beta_t$)"}
ggplot(data.table(t = window(time(y_m), start = 2000, end = c(2016, 12)),
                   y = window(freq_filt[[mods[8]]]$m[, 2],
                              start = 2000, end = c(2016, 12)
                              )),
        aes(t, y)) +
     geom_line() + 
     ylab(expression(hat(beta)[t])) +
     xlab('month') +
     scale_x_continuous(breaks = seq(2000, 2016, by = 2),
                        labels = paste('Jan', seq(2000, 2016, by = 2), sep = '\n')
                        ) +
     theme_bw()
```


# Bibliography
